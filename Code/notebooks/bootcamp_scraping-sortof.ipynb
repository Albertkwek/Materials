{"nbformat_minor": 0, "nbformat": 4, "cells": [{"source": ["# Working our way up to web scraping\n", "\n", "There's been some interest in web scraping.  It's beyond us, but there are some things we can do.  ...\n", "\n", "**Note: requires internet access to run.**  \n", "\n", "This IPython notebook was created by Dave Backus, Chase Coleman, and Spencer Lyon for the NYU Stern course [Data Bootcamp](http://databootcamp.nyuecon.com/).  "], "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": ["<a id=prelims></a>"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Preliminaries \n", "\n", "Import packages, etc.  "], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["import pandas as pd             # data package\n", "import matplotlib.pyplot as plt # graphics \n", "import sys                      # system module, used to get Python version \n", "import os                       # operating system tools (check files)\n", "import datetime as dt           # date tools, used to note current date  \n", "\n", "# these are new \n", "import requests, io             # internet and input tools  \n", "from bs4 import BeautifulSoup   # website parsing\n", "\n", "%matplotlib inline \n", "\n", "print('\\nPython version: ', sys.version) \n", "print('Pandas version: ', pd.__version__)\n", "print('Requests version: ', requests.__version__)\n", "print(\"Today's date:\", dt.date.today())"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["<a id=lucky></a>"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Sometimes we get lucky\n", "\n", "We sometimes find that we can access data straight from a web page with Pandas' `read_html`.  It works just like `read_csv` or `read_excel`.  \n", "\n", "The first example is [baseball-reference.com](http://www.baseball-reference.com/).  The same people run similar sites for football and basketball.  Many of their pages are collections of tables.  See, for example, [this one](http://www.baseball-reference.com/players/m/mccutan01.shtml) for Pittsburgh's Andrew McCucthen.    "], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# baseball reference\n", "url = 'http://www.baseball-reference.com/players/m/mccutan01.shtml'\n", "am  = pd.read_html(url)\n", "\n", "print('Ouput has type', type(am), 'and length', len(am))\n", "print('First element has type', type(am[0])')"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["**Question.** What do we have here?  A list of length 10?  Whose elements are dataframes?  Evidently this reads in all the tables from the page into dataframes and collects them in a list.  "], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["am[4].head()"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["Here's another one:  Google's stock price from Yahoo finance.  "], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["url = 'http://finance.yahoo.com/q/hp?s=GOOG+Historical+Prices'\n", "ggl = pd.read_html(url)"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["type(ggl)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["len(ggl)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["ggl[8]"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["url = 'http://databootcamp.nyuecon.com/'\n", "url = 'google.com'\n", "db  = pd.read_html(url)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["## Scanning urls"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["Itamar adds:  \n", "\n", "Walk through the following steps before running the code:\n", "\n", "1) Go to : http://finance.yahoo.com/q/hp?s=AAPL+Historical+Prices\n", "2) Enter the dates you want and hit the get prices button.\n", "3) Once the results are shown, look on the url address.\n", "4) The new url will include several parameters, each one is seperated by the & character.\n", "5) Try to explore the meanning of each parameter (s, a,b,c,d,e,f and g)\n", "6) After some trial and error you can realize that each parameter represents the data you entered as input:\n", "    the day, month and year, the stock sybmol, and the frequency  you chose (daily, weekly etc)\n", "7) Scroll down to the bottom of the page. there is a link which allows downloading the data as a csv file. click on it\n", "8) Open the CSV in excel and see the structure of the file.\n", "9) Go back to the web page, instead of clicking on the csv link, right click on it and copy the link address\n", "10) Paste the address in a notebook - This is the url link we can use to access the data from our coding environment\n", "\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["## Accessing web pages \n", "\n", "Requests again...  "], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["url = 'http://databootcamp.nyuecon.com/'\n", "db = requests.get(url)\n", "\n"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["db.headers"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["db.url"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["db.status_code"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["## Extracting pieces of web pages \n", "\n", "Use Beautiful Soup...  \n", "\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["bs = BeautifulSoup(db.content, 'lxml')\n", "\n", "print('Type and length:  ', type(bs), ', ', len(bs), sep='')\n", "print('Title: ', bs.title)\n", "print('First n characters:\\n', bs.prettify()[0:500], sep='')"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["bs.find_all?"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["bs.head"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["kids = [ c for c in bs.head.children]"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["kids"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.5.1", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}